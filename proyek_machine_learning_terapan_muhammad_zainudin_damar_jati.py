# -*- coding: utf-8 -*-
"""Proyek_Machine_Learning_Terapan_Muhammad_Zainudin_Damar_Jati.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-X4fqVeLb39l-czg4nhimxp48TYtLNfu

#Proyek Prediksi Harga Rumah di California - Muhammad Zainudin Damar Jati

Proyek ini bertujuan untuk membangun model regresi prediktif guna memperkirakan harga rumah (median house value) di wilayah California. Model dikembangkan dengan pendekatan machine learning menggunakan algoritma HistGradientBoostingRegressor, yang merupakan metode boosting efisien untuk dataset tabular skala menengah. Proyek ini dikemas dalam sebuah pipeline lengkap yang mencakup tahap praproses data, eksplorasi data (EDA), pemodelan, evaluasi, hingga prediksi terhadap data baru.

Model dibangun berdasarkan dataset California Housing, yang berisi informasi demografis dan geospasial dari berbagai blok perumahan di California.

##1. Problem Domain

Tujuan: Memprediksi harga rumah berdasarkan fitur-fitur demografi dan properti di California.

Tujuan proyek ini adalah untuk memprediksi harga rumah di California berdasarkan atribut demografis dan properti, seperti jumlah kamar, usia bangunan, populasi, dan kedekatannya terhadap laut. Karena variabel target berupa angka kontinu (median_house_value), masalah ini termasuk ke dalam regresi, bukan klasifikasi. Memahami domain masalah ini penting agar pendekatan dan metrik evaluasi yang digunakan selaras dengan tujuan prediksi.

##2. Import Library

### Import Library dan Konfigurasi Awal

Pada bagian ini, kita mengimpor berbagai pustaka yang dibutuhkan untuk proses pemodelan Machine Learning, serta melakukan beberapa konfigurasi awal:

- `warnings.filterwarnings("ignore", category=FutureWarning)`  
  Menonaktifkan peringatan tipe *FutureWarning* agar output lebih bersih.


#### Pustaka Dasar Python dan Visualisasi

- `time`, `math` – untuk pengukuran waktu dan operasi matematika.
- `numpy`, `pandas` – manipulasi array dan data tabular.
- `seaborn`, `matplotlib.pyplot` – visualisasi data.


#### Modul Scikit-learn

- `train_test_split`, `RandomizedSearchCV`, `cross_val_score` – pembagian data dan validasi model.
- `HistGradientBoostingRegressor` – algoritma ensemble boosting untuk regresi.
- `mean_squared_error`, `r2_score`, `mean_absolute_error` – metrik evaluasi model.
- `MinMaxScaler`, `OneHotEncoder` – preprocessing fitur.
- `SimpleImputer`, `ColumnTransformer`, `Pipeline` – pipeline dan penanganan data yang hilang.


- `files` – dari `google.colab`, digunakan untuk mengunggah atau mengunduh file dari dan ke Google Colab.

- `sns.set_theme(style="darkgrid")`  
  Mengatur tema default visualisasi Seaborn dengan tampilan bergrid gelap.
"""

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

import time, math
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

from google.colab import files
sns.set_theme(style="darkgrid")

"""##3. Data Understanding

###Mengunggah dan Membaca Dataset

Pada tahap awal ini, kita akan mengunggah dan membaca data California Housing menggunakan Google Colab dan pustaka `pandas`.

* `files.upload()`
  Digunakan untuk mengunggah file dari komputer lokal ke lingkungan Google Colab.

* `pd.read_csv("housing.csv")`
  Membaca file CSV dan memuatnya ke dalam DataFrame `df` untuk analisis lebih lanjut.
"""

uploaded = files.upload()
df = pd.read_csv("housing.csv")

"""### Deskripsi Dataset

Dataset yang digunakan merupakan bagian dari **California Housing data** yang berisi informasi dari sensus penduduk California tahun 1990. Setiap baris merepresentasikan satu blok (kawasan perumahan kecil), dengan berbagai fitur demografis dan perumahan.

| Fitur                | Tipe        | Deskripsi                                              |
| -------------------- | ----------- | ------------------------------------------------------ |
| `longitude`          | numerik     | Koordinat geografis garis bujur                        |
| `latitude`           | numerik     | Koordinat geografis garis lintang                      |
| `housing_median_age` | numerik     | Umur median bangunan di blok tersebut                  |
| `total_rooms`        | numerik     | Total jumlah ruangan di blok                           |
| `total_bedrooms`     | numerik     | Total jumlah kamar tidur                               |
| `population`         | numerik     | Jumlah penduduk di blok                                |
| `households`         | numerik     | Jumlah keluarga atau unit rumah tangga                 |
| `median_income`      | numerik     | Pendapatan rata-rata penduduk (dalam puluhan ribu USD) |
| `ocean_proximity`    | kategorikal | Kedekatan blok terhadap laut                           |
| `median_house_value` | numerik     | **Target:** Nilai median rumah di blok (dalam USD)     |

Dataset ini akan digunakan untuk membangun model regresi dalam rangka memprediksi harga median rumah berdasarkan fitur-fitur demografis dan geografis yang tersedia.

### Menampilkan Sampel Data

Menampilkan 5 baris pertama dari dataset untuk memberikan gambaran umum tentang struktur dan isi data.
"""

print("Sample data:")
display(df.head())

"""### Informasi Struktur Dataset

Menampilkan informasi umum mengenai dataset, seperti jumlah entri, jumlah kolom, tipe data, serta jumlah nilai non-null pada setiap kolom.

"""

print("Informasi Data:")
df.info()

"""### Mengecek Nilai Hilang

Menghitung jumlah nilai yang hilang (missing values) pada masing-masing kolom dalam dataset. Hal ini penting untuk mengetahui apakah perlu dilakukan imputasi atau pembersihan data sebelum pemodelan.

"""

print("\nJumlah Nilai Hilang:")
print(df.isna().sum())

"""##4. Data Preparation

### Penanganan Nilai Hilang dan Teknik Rekayasa Fitur

Pada tahap ini, kita melakukan imputasi nilai hilang dan menambahkan fitur baru untuk meningkatkan kualitas model prediktif.
"""

df["total_bedrooms"] = df["total_bedrooms"].interpolate()

"""### Rekayasa Fitur (Feature Engineering)

Ditambahkan tiga fitur baru yang dapat memberikan insight lebih dalam terkait pola perumahan dan kepadatan:

* `rooms_per_household`: Rasio jumlah ruangan terhadap jumlah rumah tangga.
* `bedrooms_per_room`: Rasio kamar tidur terhadap jumlah ruangan.
* `population_per_household`: Rasio jumlah penduduk terhadap jumlah rumah tangga.

"""

df["rooms_per_household"] = df["total_rooms"] / df["households"]
df["bedrooms_per_room"] = df["total_bedrooms"] / df["total_rooms"]
df["population_per_household"] = df["population"] / df["households"]

"""### Pembersihan Outlier dengan IQR Clipping

Untuk menjaga distribusi data tetap sehat dan mencegah outlier ekstrem memengaruhi model, kita gunakan teknik **IQR clipping** pada semua fitur numerik:

* `IQR` (Interquartile Range) dihitung untuk setiap fitur.
* Nilai di luar batas bawah (`Q1 - 1.5 * IQR`) dan atas (`Q3 + 1.5 * IQR`) dipotong.

"""

def iqr_clip(df, columns):
    for col in columns:
        Q1, Q3 = df[col].quantile([0.25, 0.75])
        IQR = Q3 - Q1
        lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR
        df[col] = df[col].clip(lower, upper)
    return df

num_features = df.select_dtypes(include=[np.number]).columns
df = iqr_clip(df, num_features)

"""### Visualisasi Distribusi Fitur Numerik

Distribusi semua fitur numerik divisualisasikan menggunakan histogram dengan KDE (*Kernel Density Estimation*) untuk membantu memahami bentuk distribusinya.

* `sns.histplot()` digunakan untuk menampilkan histogram dan kurva distribusi.
* Visualisasi dibagi ke dalam grid berdasarkan jumlah fitur.

"""

plt.figure(figsize=(16, 12))
for i, col in enumerate(num_features, 1):
    plt.subplot(math.ceil(len(num_features)/3), 3, i)
    sns.histplot(df[col], bins=30, kde=True, color='tab:blue')
    plt.title(f"Distribusi {col}")
plt.tight_layout()
plt.show()

"""### Korelasi antar Fitur Numerik

Digunakan heatmap korelasi untuk mengidentifikasi hubungan linear antar fitur numerik. Informasi ini berguna untuk:

* Menemukan fitur yang redundant.
* Menghindari multikolinearitas.
* Memahami pengaruh antar variabel.

"""

plt.figure(figsize=(10, 8))
sns.heatmap(df[num_features].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Korelasi antar Fitur Numerik")
plt.show()

"""##5. Modeling

### Persiapan Fitur dan Target

Pada tahap ini, kita memisahkan fitur (X) dan target (y), serta menerapkan transformasi logaritmik pada target untuk mengurangi efek skewness dan menangani distribusi target yang tidak normal.

* `X = df.drop(columns="median_house_value")`
  Menghapus kolom target dari dataset untuk dijadikan fitur input.

* `y = np.log1p(df["median_house_value"])`
  Menerapkan transformasi log(1 + x) pada target `median_house_value` untuk memperhalus distribusi data target.
"""

X = df.drop(columns="median_house_value")
y = np.log1p(df["median_house_value"])

"""### Identifikasi Tipe Fitur

Kita mengelompokkan fitur ke dalam dua kategori:

* `numeric_features`: Fitur bertipe numerik.
* `categorical_features`: Fitur bertipe kategorikal.

"""

numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = X.select_dtypes(exclude=[np.number]).columns.tolist()

"""### Pipeline Pranala (Preprocessing)

Pipeline preprocessing digunakan untuk mempersiapkan data sebelum masuk ke model.

#### Pipeline Numerik

* `SimpleImputer(strategy="median")`: Mengisi nilai kosong menggunakan nilai median.
* `MinMaxScaler()`: Menskalakan nilai agar berada dalam rentang 0–1.
"""

numeric_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", MinMaxScaler())
])

"""#### Pipeline Kategorikal

* `OneHotEncoder(drop="first")`: Mengubah nilai kategorikal menjadi vektor numerik, drop level pertama untuk menghindari dummy variable trap.



"""

categorical_pipeline = Pipeline([
    ("encoder", OneHotEncoder(drop="first", sparse_output=False, handle_unknown="ignore"))
])

"""#### ColumnTransformer

Menggabungkan pipeline numerik dan kategorikal ke dalam satu objek transformasi:


"""

preprocessor = ColumnTransformer([
    ("num", numeric_pipeline, numeric_features),
    ("cat", categorical_pipeline, categorical_features)
])

"""###  Pipeline Pemodelan

Model yang digunakan adalah **`HistGradientBoostingRegressor`**, sebuah algoritma ensemble boosting yang efisien untuk data tabular:

* Pipeline menggabungkan proses preprocessing dan pelatihan model menjadi satu alur terintegrasi.
* `set_output(transform="pandas")` memungkinkan output pipeline dalam bentuk DataFrame.
"""

model_pipeline = Pipeline([
    ("preprocessing", preprocessor),
    ("regressor", HistGradientBoostingRegressor(random_state=42))
])
model_pipeline.set_output(transform="pandas")

"""###  Pembagian Data Training dan Test

Data dibagi menjadi dua subset untuk pelatihan dan evaluasi:

* `test_size=0.2`: 20% data digunakan sebagai data uji.
* `random_state=42`: Menjaga konsistensi hasil pembagian.

"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""###  Hyperparameter Tuning dengan RandomizedSearchCV

Dilakukan pencarian parameter terbaik menggunakan teknik Randomized Search:

* `param_dist`: Ruang parameter yang dieksplorasi.
* `n_iter=10`: Jumlah kombinasi parameter yang dicoba.
* `cv=3`: 3-fold cross-validation.
* `scoring='r2'`: Metrik evaluasi yang digunakan.
* `n_jobs=-1`: Menggunakan seluruh core CPU untuk percepatan proses.

"""

param_dist = {
    'regressor__max_iter': [100, 200, 300],
    'regressor__max_leaf_nodes': [31, 50, 100],
    'regressor__learning_rate': [0.05, 0.1, 0.2]
}

random_search = RandomizedSearchCV(
    model_pipeline, param_distributions=param_dist,
    n_iter=10, cv=3, scoring='r2', n_jobs=-1, random_state=42, verbose=1
)

"""### Training Model dan Output Parameter Terbaik

Model dilatih menggunakan data training, dan waktu pelatihan dicatat. Setelah pelatihan selesai, ditampilkan parameter terbaik dari hasil pencarian.

"""

start = time.time()
random_search.fit(X_train, y_train)
print(f"\n>> Waktu training: {time.time() - start:.2f} detik")
print("\nParameter terbaik:")
print(random_search.best_params_)

"""##6. Evaluation

### Evaluasi Model Terbaik

Setelah proses pelatihan dan tuning, model terbaik dari `RandomizedSearchCV` diekstrak dan digunakan untuk membuat prediksi pada data uji (`X_test`). Karena target semula ditransformasi menggunakan `log1p`, maka hasil prediksi dan target dikembalikan ke skala aslinya menggunakan `expm1`.
"""

best_model = random_search.best_estimator_
y_pred_log = best_model.predict(X_test)
y_pred = np.expm1(y_pred_log)
y_test_exp = np.expm1(y_test)

"""
### Metode Evaluasi Kinerja Model

Fungsi `evaluate_model()` menghitung dan menampilkan metrik evaluasi regresi:

* **R² Score** – seberapa baik model menjelaskan variasi data.
* **RMSE** – Root Mean Squared Error.
* **MAE** – Mean Absolute Error.
"""

def evaluate_model(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = math.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)

    print("\n=== Evaluasi Model ===")
    print(f"R² Score : {r2:.4f}")
    print(f"RMSE     : {rmse:.4f}")
    print(f"MAE      : {mae:.4f}")

evaluate_model(y_test_exp, y_pred)

"""### Visualisasi: Prediksi vs Nilai Aktual

Scatter plot berikut memperlihatkan hubungan antara prediksi dan nilai aktual:

* Titik-titik dekat garis merah putus-putus menandakan prediksi yang mendekati nilai aktual.
* Penyimpangan dari garis tersebut menunjukkan error prediksi.

"""

plt.figure(figsize=(8, 6))
sns.scatterplot(x=y_test_exp, y=y_pred, alpha=0.5, color='tab:blue')
plt.plot([y_test_exp.min(), y_test_exp.max()], [y_test_exp.min(), y_test_exp.max()], '--r')
plt.xlabel("Harga Aktual")
plt.ylabel("Harga Prediksi")
plt.title("Prediksi vs Realita Harga Rumah")
plt.show()

"""### Visualisasi: Distribusi Residual

Fungsi `plot_residuals()` digunakan untuk melihat distribusi selisih antara nilai aktual dan prediksi:

* Residual = `y_true - y_pred`
* Idealnya, residual terdistribusi normal dan terpusat di sekitar nol.

"""

def plot_residuals(y_true, y_pred):
    residuals = y_true - y_pred
    plt.figure(figsize=(8, 5))
    sns.histplot(residuals, bins=40, kde=True, color='tab:orange')
    plt.title("Distribusi Residual")
    plt.xlabel("Residual")
    plt.ylabel("Frekuensi")
    plt.axvline(0, color='black', linestyle='--')
    plt.show()

plot_residuals(y_test_exp, y_pred)

"""### Validasi Silang (Cross-validation)

Dilakukan validasi silang sebanyak 5 lipatan (`5-fold`) pada keseluruhan data untuk menilai stabilitas dan generalisasi model:

"""

cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='r2')
print(f"\nCross-validation R² Score (5-fold): {cv_scores.mean():.4f} ± {cv_scores.std():.4f}")

"""### Prediksi Harga Rumah Baru

Model juga digunakan untuk memprediksi harga rumah baru dengan spesifikasi tertentu. Fitur-fitur numerik tambahan seperti `rooms_per_household`, `bedrooms_per_room`, dan `population_per_household` dihitung secara manual sesuai formula preprocessing.

"""

new_house = pd.DataFrame([{
    "longitude": -118.0,
    "latitude": 34.0,
    "housing_median_age": 30,
    "total_rooms": 2000,
    "total_bedrooms": 400,
    "population": 800,
    "households": 300,
    "median_income": 5.0,
    "ocean_proximity": "NEAR OCEAN",
    "rooms_per_household": 2000 / 300,
    "bedrooms_per_room": 400 / 2000,
    "population_per_household": 800 / 300
}])

predicted_price = np.expm1(best_model.predict(new_house)[0])
print("\n=== Prediksi Harga Rumah Baru ===")
print(f"Spesifikasi rumah:\n{new_house.to_string(index=False)}")
print(f"\n>> Prediksi harga rumah: ${predicted_price:,.2f}")